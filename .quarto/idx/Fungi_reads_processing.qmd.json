{"title":"Processing of ITS2 reads obtained from Illumina MiSeq platform","markdown":{"yaml":{"title":"Processing of ITS2 reads obtained from Illumina MiSeq platform","format":{"html":{"toc":true}},"execute":{"eval":false},"editor_options":{"chunk_output_type":"console"}},"headingText":"1. Formatting the name of the samples","containsRefs":false,"markdown":"\n\nFirst of all, we will install and load of the required packages and libraries\n\n```{r}\ninstall.packages(\"devtools\")\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\")\ndevtools::install_github(\"nuriamw/micro4all\")\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ShortRead\")\ninstall.packages(\"tidyverse\")\n\nlibrary(devtools)\nlibrary(dada2)\nlibrary(micro4all)\nlibrary(ShortRead)\nlibrary(tidyverse)\n```\n\nAs you can see, we are going to use different functions of [DADA2 package](https://benjjneb.github.io/dada2/tutorial.html \"Click to DADA2 website\"), so it will be nice if you read all the documentation regarding this package.\n\n::: callout-important\nThe steps 1 to 7 must be performed for each sequencing run. If you are working with different sequencing runs, you have to join all the sequences tables into one object (step 8). Once the joined element is obtained, steps 9-last step have to be applied to the joined object.\n:::\n\n::: callout-warning\nIf you have included a Mock community which does not include fungal sequences (or a very small number of fungal sequences) in your sequencing run, I suggest you to process first the *16S rRNA* dataset.\n:::\n\n\nNow, we will start! Set the working directory and specify the path where you will be working:\n\n```{r}\npath= \"~/Platano_PLEC/PLEC_muestreo2022/310523_ITS/reads\"\nlist.files(path) #Check that all files are here included (script + fastq files)\n```\n\nSort F and R reads separately and save them into two variables:\n\n```{r}\nfnFs = sort(list.files(path, pattern=\"_R1_001.fastq.gz\", full.names = TRUE))\nfnRs = sort(list.files(path, pattern=\"_R2_001.fastq.gz\", full.names = TRUE))\n#our filenames have the format \"NGS015-23-ITS2-A2S12R_S51_L001_R2_001.fastq.gz\"\n```\n\nExtract the name (code) of our samples, and remove all the extra information from the filenames:\n\n```{r}\nsample.names_raw = sapply(strsplit(basename(fnFs), \"_\"), `[`, 1) \n#split the string when \"_\" is found and keep the 4th part of the string\n#we will get, for example, this: NGS015-23-ITS2-A2S12R\n\nsample.names= gsub(patter = \"NGS015-23-ITS2-\", replacement = \"\", sample.names)#here we replace the extra information by nothing (no characters)\nsample.names\n```\n\n::: callout-note\nNote that in this case, we have modified the sample names by employing a different way from that used for bacterial dataset. However, the result is exactly the same.\n:::\n\n# 2. Check the quality of the sequencing\n\nThere are different ways to check the quality of the reads:\n\n## a) Count the number of reads\n\nIt would be nice to check whether we obtained enough reads from the sequencing service. Otherwise, we should ask for the service to repeat the sequencing.\n\n```{r}\nraw_reads_count = NULL\n\nfor (i in 1:length(fnFs)){\n    raw_reads_count = rbind(raw_reads_count, \n                            length(ShortRead::readFastq(fnFs[i])))\n  } #this loop counts the number of F reads by means of the ShortRead package\n\nrownames(raw_reads_count)= sample.names #formatting of the output\ncolnames(raw_reads_count)= \"Number_of_reads\"\na=data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count)\n\nraw_reads_count2 = NULL\nfor (i in 1:length(fnRs)){#do the same with R reads\n  raw_reads_count2 <- rbind(raw_reads_count2, \n                            length(ShortRead::readFastq(fnRs[i])))\n} \nrownames(raw_reads_count2)= sample.names\ncolnames(raw_reads_count2)= \"Number_of_reads\"\nb=data.frame(\"_\"=rownames(raw_reads_count2),raw_reads_count2)\n\na==b #check that we get the same number of F and R reads\n\ncbind(row.names(raw_reads_count)[which.min(raw_reads_count)],\n      min(raw_reads_count))#check which sample accounts for the lowest number of reads. Be careful if you keep the negative control of the sequencing\ncbind(row.names(raw_reads_count)[which.max(raw_reads_count)],\n      max(raw_reads_count))#check which sample accounts for the highest number of reads.\n\nwrite.table(data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count),\n            file=\"Num_raw_reads_ITS.txt\", sep=\"\\t\",row.names =F)\nView(raw_reads_count) #please, check the number of reads per sample\n```\n\nAt this point, we have to check the number of raw reads per sample. The researcher has to verify whether this number is enough or not. It depends on the type of sample, among other parameters.\n\n## b) Inspect the length of the reads\n\n::: callout-important\nRemember that ITS2 varies in length among different fungi. We have to keep this in mind along the analysis, since we have to skip several steps that are based on the length of the reads (comparing to the processing of *16S rRNA* reads).\n:::\n\nIn spite of that indicated in the above callout, we will inspect the length of the reads just to confirm the genomics service has performed a good job.\n\nIn our case, the genomics service followed a 2x300 bp PE strategy.\n\n```{r}\nreads=ShortRead::readFastq(fnFs) #save the reads in a new variable with the format of ShortRead package, which is a bit different from standard R variable\nuniques = unique(reads@quality@quality@ranges@width) #get the length of the reads\n\ncounts= NULL\n\nfor (i in 1:length(uniques)) {\n  counts=rbind(counts,\n               length(which(reads@quality@quality@ranges@width==uniques[i])))\n  \n}#specific loop to count the number of reads of each length.  \n\nhistogram =  cbind(uniques,counts)\ncolnames(histogram) = c(\"Seq.length\", \"counts\")\n\n#check the histogram\nhead(histogram[order(histogram[,1],decreasing = TRUE),]) #Most of the sequences should fall in expected sequence length\n\n#plotting\nhist(reads@quality@quality@ranges@width, main=\"Forward length distribution\",\n     xlab=\"Sequence length\", ylab=\"Raw reads\")\n\nwrite.table(histogram, file=\"Lenght_raw_reads_ITS.txt\", sep=\"\\t\",row.names =F)\n```\n\n## c) Check the quality plots\n\nCheck if the overlapping of F and R reads is possible considering the quality of the last nucleotides. Be careful with the quality of R reads (which tends to be worse than that of F reads, especially in the latest part of the reads).\n\n```{r}\nplotQualityProfile(fnFs[4:5])#select the specific samples you want to check, in this case 4 and 5\nplotQualityProfile(fnRs[4:5])\n```\n\n# 3. Filter and trimming step\n\n::: callout-note\nIf you come from the processing of bacterial reads, you would have noticed that here we have skipped the step of FIGARO. That tool needs that all the reads are of the same length, so we cannot use it for fungal (ITS2) dataset.\n:::\n\n```{r}\nfiltFs=file.path(path, \"filtered\", basename(fnFs))#create the \"filtered\" directory\nfiltRs=file.path(path, \"filtered\", basename(fnRs))\n```\n\nSince we cannot use FIGARO, we have to test different parameters in the following filtering and trimming step. It is recommended to achieve a compromise between the percentage of retained sequences and their quality. So, run the following command indicating specific values of maxEE according to the quality plots of F and R reads previously visualized in the **step 2.c)**. Write down the results obtained for that maxEE, and then re-run with another maxEE values, until you reach to the best solution.\n\n```{r}\nout=filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN=0, maxEE=c(3,4), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE, minLen=50)\n\n\n#maxN=0 : remove the reads with at least 1 ambiguity\n#trunQ=2: remove all the reads with one nucleotide with 2 or less quality value\n#maxEE: maximum expected error for F and R reads\n#minLen=50: remove reads shorter than 50 bp\n\nhead(out)#important! Check the results\n```\n\nIn our case, we selected:\n\n| Sequencing run | Positions |       | maxEE |       |\n|----------------|-----------|-------|-------|-------|\n|                | **F**     | **R** | **F** | **R** |\n| **Run5**       |           |       | 3     | 4     |\n| **Run6**       |           |       | 3     | 4     |\n| **Run7**       |           |       | 4     | 6     |\n\n::: callout-note\nIf you come from the processing of bacterial reads, you would have noticed that in the above code we have not specified the trimming positions. This is not a mistake; we do not indicate the trimming position since the ITS2 region is variable in length and we will keep all the sequences regardless of their length.\n:::\n\nOur reads are partially filtered and trimmed (still have the primers!).\n\n# 4. Cutadapt: removal of the primers\n\nOur reads still have the primers (which are artificial sequences and can have ambiguities). So, we have to remove them and discard all the sequences in which primers are not found (because if the sequencing run was ok, primers should had been found inside the reads). Let's use cutadapt tool, which does not run in R but in python:\n\n```{r}\nFWD = \"GTGARTCATCGAATCTTTG\" #sequence of F primer\nREV = \"TCCTCCGCTTATTGATATGC\" #sequence of R primer\n```\n\n```{r}\nallOrients = function(primer) {\n  require(Biostrings)\n  dna =DNAString(primer)  #package Biostrings does not work with characters but with strings, so let's change the class of the primers variables.\n  orients = c(Forward = dna, Complement = Biostrings::complement(dna), \n              Reverse = reverse(dna), RevComp = reverseComplement(dna))\n  return(sapply(orients, toString)) #change from string to character\n} #this function calculates all the possible orientations of the primers\n\nFWD.orients = allOrients(FWD) #pass the function to F and R primers\nREV.orients  = allOrients(REV)\nFWD.orients\nREV.orients\n```\n\n```{r}\nprimerHits =function(primer, fn) {\n  nhits =vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n  return(sum(nhits > 0))\n} #it counts the number of sequence in each orientation\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = filtFs[[4]]),#check for instante, in sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = filtRs[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = filtFs[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = filtRs[[4]]))\n#here we make a summary table. Please, visit DADA2 website for a better interpretation of the table.\n```\n\nNow yes, we are running cutadapt, although we have to adapt the reads previously for cutadapt. This tool need some flags that have to be added to the reads. Visit the web of [cutadapt](https://cutadapt.readthedocs.io/en/stable/guide.html \"User guide of cutadapt\").\n\n```{r}\ncutadapt  =  \"/usr/bin/cutadapt\" #path to cutadapt \n\nsystem2(cutadapt, args = c(\"--version\")) # Run shell commands from R\n\npath.cut =file.path(path, \"cutadapt\") #create a directory where processed reads will be saved\n\nif(!dir.exists(path.cut)) dir.create(path.cut)\n\nfnFs.cut =file.path(path.cut, basename(filtFs))#if you came from the processing of 16S rRNA reads, be careful here. We have to use \"filtFs\" instead of \"fnFs\" because we have already performed the filtering and trimming steps.\nfnRs.cut =file.path(path.cut, basename(filtRs))#the same comment here\n\n#Produce arguments for cutadapt\nFWD.RC = dada2:::rc(FWD)  \nREV.RC = dada2:::rc(REV)\n\nR1.flags  =  paste0(\"-a\", \" \", \"^\",FWD,\"...\", REV.RC) #adding the flags\nR2.flags  =  paste0(\"-A\",\" \",\"^\", REV, \"...\", FWD.RC)\n\n#run cutadapt\nfor(i in seq_along(fnFs)) {\n  system2(cutadapt, args = c(R1.flags, R2.flags, \"-n\", 2,\"-m\", 1, \"--discard-untrimmed\", \"-j\",0, \"-o\", fnFs.cut[i], \"-p\", fnRs.cut[i], filtFs[i], filtRs[i],\"--report=minimal\")) \n}\n\n#-n 2: remove the primers\n#-m 1: remove empty sequences\n#-j 0: automatically detect number of cores\n#-0: output files\n#-i: input files. We use the filtered and trimmed reads\n```\n\nNow, we check whether cutadapt has removed all the primers. For that purpose, we pass the previously created function to check the number of times each primer appears in our dataset:\n\n```{r}\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[4]]),#Here we can indicate the number of the sample we want to check. In this case, sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[4]]))\n```\n\n::: callout-note\nNote that in some cases, after the removal of the primers by cutadapt, the function *primerHits* still finds some primers in our dataset. Do no panic, these differences are normal because cutadapt and that DADA2 functions are based on different algorithms.\n:::\n\n# 5. DADA2: machine learning\n\nWe are going to use DADA2 to denoise, infer the samples and to correct the expected errors by applying different functions of the package.\n\n```{r}\n# First, learn error rates\nerrF =learnErrors(fnFs.cut, multithread=T, verbose=1)\nerrR =learnErrors(fnRs.cut, multithread=T, verbose=1)\n\n# Lets plot the errors in each position\nplotErrors(errF, nominalQ=TRUE)\nplotErrors(errR, nominalQ=TRUE)\n#these plots show the possible errors made for each possible mutation (A>C, A>G, etc). Dots should be close to the red line.\n```\n\n# 6. DADA2: Sample inference\n\n```{r}\ndadaFs = dada(fnFs.cut, err=errF, multithread=TRUE)\ndadaRs = dada(fnRs.cut, err=errR, multithread=TRUE)\ndadaFs[[4]]#check the inferred fourth sample (it's just an example). We will obtain the number of true sequence variants from the X number of unique sequences.\n\n# Set sample names\nnames(dadaFs) = sample.names\nnames(dadaRs) = sample.names\n```\n\n# 7. Overlap F and R sequences\n\nIn this step we are going to overlap F (forward) and R (reverse) reads coming from the same sample.\n\n```{r}\nmergers=mergePairs(dadaFs, fnFs.cut, dadaRs, fnRs.cut, verbose=TRUE)\nhead(mergers[[4]]) #check the output for sample number 4\n```\n\nNow, we are going to construct an amplicon sequence variant table (ASV table).\n\nBe careful, because we will get one ASV table per sequencing run. Thus, as we got 3 runs for fungal amplicons, we will get 3 different ASV tables.\n\n```{r}\nseqtab_run5= makeSequenceTable(mergers)\ndim(seqtab_run5) #indicates the number of samples (including negative controls) and the number of ASVs\nsaveRDS(seqtab_run5, \"~/Platano_PLEC/PLEC_muestreo2022/310523_ITS/reads/seqtab_run5.rds\") #save the seqtab in .rds format\n```\n\n# 8. Merge sequence tables from different runs\n\nAs indicated before, in this step we are going to merge the sequence tables (\"seqtabs\") coming from different runs. So, firstly, we have to load all of them to the current project. Please, note that all the seqtabs have to be in the same working directory!\n\n```{r}\nseqtab_run5=readRDS(\"seqtab_run5.rds\")#load all the .rds files in the same directory\nseqtab_run6=readRDS(\"seqtab_run6rds\")\nseqtab_run7=readRDS(\"seqtab_run7rds\")\n\n\nmergedSeqTab=mergeSequenceTables(seqtab_run5,seqtab_run6,seqtab_run7,repeats=\"sum\")\n#with \"sum\" we indicate that we want to join and sum all the reads that are in samples with the same name. Why do we do that? \n```\n\n# 9. Chimeral removal\n\nAs the sequencing process is based on PCRs, chimeric sequences are expect to appear. So, let's remove them.\n\n```{r}\nseqtab.nochim = removeBimeraDenovo(mergedSeqTab, method=\"consensus\", multithread=TRUE, verbose=TRUE)#if you just are working on one dataset (coming from just one sequencing run, instead of \"mergedSeqTab\", you have to write \"seqt_tab\" or the name of your seqtab)\ndim(seqtab.nochim) #indicates the number of samples and ASVs (but not the number of sequences)\n```\n\n# 10. Other steps: flter the ASVs by their length\n\nLet's check the number of ASVs and their length:\n\n```{r}\ntable(nchar(getSequences(seqtab.nochim)))  #Number of ASV of each length\n  #first line: length of the sequences\n  #second line: number of sequences of each length\n```\n\nCalculate the number of sequences of each ASV that of each corresponding length:\n\n```{r}\nreads.per.seqlen = tapply(colSums(seqtab.nochim), factor(nchar(getSequences(seqtab.nochim))), sum)\nreads.per.seqlen\n\ntable_reads_seqlen = data.frame(length=as.numeric(names(reads.per.seqlen)), count=reads.per.seqlen)\nggplot(data=table_reads_seqlen, aes(x=length, y=count)) + geom_col()\n```\n\n# 11. Taxonomic classification\n\nWe are going to classify our ITS2 amplicons against [UNITE](https://doi.org/10.15156/BIO/2938067 \"Click to the paper\") v.9.0 database, which is host in our PC or server.\n\n```{r}\ntaxa_unite =assignTaxonomy(seqtab.nochim, \"/home/databases/sh_general_release_dynamic_25.07.2023.fasta\", multithread=TRUE)#indicate the path were your database is located\n\nASV = seqtab.nochim #edit the tables\nASVt = t(ASV)\n\ntaxa_na = apply(taxa_unite,2, tidyr::replace_na, \"unclassified\")[,-7] #replace the \"NA\" values by \"unclassified\" in those case in which an ASV have not been identified at a specific taxonomic rank\ntaxa_rdp_na=taxa_rdp_na[,-(7:15)]\n\n#we are going to modify a bit the tables so that the name of ASVs looks like \"ASV00001\" (in case we have 10000 ASVs or more). In case we have just 100 ASVs, they will have that code: \"ASV001\"\n\nnumber.digit = nchar(as.integer(nrow(ASVt)))\nnames =paste0(\"ASV%0\", number.digit, \"d\") #As many 0 as digits\nASV_names<- sprintf(names, 1:nrow(ASVt))\n\nASV_table_classified_raw = cbind(as.data.frame(taxa_na,stringsAsFactors = FALSE),as.data.frame(ASV_names, stringsAsFactors = FALSE),as.data.frame(ASVt,stringsAsFactors = FALSE))\n\nASV_seqs = rownames(ASV_table_classified_raw)\nrownames(ASV_table_classified_raw) <- NULL\nASV_table_classified_raw = cbind(ASV_seqs, ASV_table_classified_raw)\n\nwrite.table(ASV_table_classified_raw, file=\"ASV_table_classified_raw_wirhMockwithPlastids.txt\", sep=\"\\t\")\n```\n\nNow, we have our ASV table with the corresponding taxonomic classification. But, it still potentially includes some sequences that could be erroneous.\n\n# 12. MOCK Community: setting the sequencing detection limit\n\nThe Mock Community we used in our sequencing runs just includes sequences of two fungi (yeast indeed). Thus, we will consider that the detection limit previously established for bacterial dataset is the same as for fungi. Ours was **0.001207%** of the sequences. Let's use it:\n\n```{r}\nASV_sums=rowSums(ASV_table_classified_raw[,9:ncol(ASV_table_classified_raw)])\nsum.total=sum(ASV_sums)# Get the total number of sequences\nnseq_cutoff=(0.001207/100)*sum.total# Apply the percentage to sequence number\n\n# Now, filter the table accordingly:\nASV_filtered=ASV_table_classified_raw[which(ASV_sums>nseq_cutoff),] #we are retaining just those ASVs accounting for more sequences that that determined by the cut-off\nASV_table=ASV_filtered[order(ASV_filtered[[\"ASV_names\"]]),]# Sort table in ascending order of ASV names\n```\n\n# 13. Removal of erronous sequences and taxonomy refinement\n\nLet's remove all the undesired sequences (if we are working with plant endophytes, we will have retained plant sequences, e.g., chloroplasts). We will also remove those ASVs not classified even at kingdom level.\n\n```{r}\nASV_final=ASV_table[(which(ASV_table$Kingdom!=\"unclassified\")),] #remove all the sequences not classified at Kingdom level\n```\n\nSometimes, when we have worked with plant tissues, we can find some fungi that are ascribed to \"*incertae sedis*\" group (missclassified) even at phylum level. They tend to be sequences of the plant host. So, let's verify it and remove them in case we find these sequences:\n\n```{r}\nincertae_phy=ASV_final[which(ASV_final$Phylum==\"p__Fungi_phy_Incertae_sedis\"),] #UNITE database writes \"p__\" prior to the name of the phyla\nseq_incertae=as.list(incertae_phy$ASV_seqs)\nwrite.fasta(seq_incertae, names=incertae_phy$ASV_names, file.out=\"incertaesedis_phylum.fas\", open = \"w\", nbchar =1000 , as.string = FALSE)#save the fasta file of these ASVs\n\n#Let's perform a comparison of these sequences against the NCBI GenBank database by BLASTn\nsystem((\"/home/programas/ncbi-blast-2.13.0+/bin/blastn -query incertaesedis_phylum.fas -db /home/databases/nt -out incertae_phylum_hits.txt -outfmt '6 std stitle' -show_gis -max_target_seqs 20 -parse_deflines -num_threads 10\"),intern = TRUE)\n\n#The output of the BLAST (\"incertae_phylum_hits.txt\") have to be manually checked.\n\n#In our case, all the sequences are related to *Musa* spp. and other eukaryots that are not fungi, so we are going to remove all these sequences:\n\nASV_final2 = ASV_final[(which(ASV_final$Phylum!=\"p__Fungi_phy_Incertae_sedis\")),]\n```\n\nRepeat the same with the unclassified sequences at phylum level:\n\n```{r}\nunclassified_phy=ASV_final2[which(ASV_final2$Phylum==\"unclassified\"),]\nseq_unclassified= as.list(unclassified_phy$ASV_seqs)\nwrite.fasta(seq_unclassified, names=unclassified_phy$ASV_names, file.out=\"unclassified_phylum.fas\", open = \"w\", nbchar =1000 , as.string = FALSE)\nsystem((\"/home/programas/ncbi-blast-2.13.0+/bin/blastn -query unclassified_phylum.fas -db /home/databases/nt -out unclassified_phylum_hits.txt -outfmt '6 std stitle' -show_gis -max_target_seqs 5 -parse_deflines -num_threads 10\"),intern = TRUE)\n\n#We found a lot of missclassified ASVs at phylum, so we are going to remove them especifically:\n\nASV_final3 = ASV_final2[(which(ASV_final2$ASV_names!=\"ASV0004\"&\n                                ASV_final2$ASV_names!=\"ASV0012\"&\n                                ASV_final2$ASV_names!=\"ASV0109\"&\n                                ASV_final2$ASV_names!=\"ASV0140\"&\n                                ASV_final2$ASV_names!=\"ASV0269\"&\n                                ASV_final2$ASV_names!=\"ASV0627\"&\n                                ASV_final2$ASV_names!=\"ASV0654\"&\n                                ASV_final2$ASV_names!=\"ASV1112\"&\n                                ASV_final2$ASV_names!=\"ASV0938\"&\n                                  ASV_final2$ASV_names!=\"ASV0911\"&\n                                  ASV_final2$ASV_names!=\"ASV0731\"&\n                                  ASV_final2$ASV_names!=\"ASV0844\"&\n                                  ASV_final2$ASV_names!=\"ASV0923\"&\n                                  ASV_final2$ASV_names!=\"ASV1080\"&\n                                 ASV_final2$ASV_names!=\"ASV1096\"&\n                                 ASV_final2$ASV_names!=\"ASV1147\"&\n                                 ASV_final2$ASV_names!=\"ASV1291\"&\n                                 ASV_final2$ASV_names!=\"ASV1321\"&\n                                 ASV_final2$ASV_names!=\"ASV1335\")),]\n\n```\n\nEventually, save the definitive ASV table:\n\n```{r}\nwrite.table(data.frame(\" \"=rownames(ASV_final3),ASV_final3),file=\"ASV_Hongos_FINAL.txt\", sep=\"\\t\",row.names =F)\n```\n","srcMarkdownNoYaml":"\n\nFirst of all, we will install and load of the required packages and libraries\n\n```{r}\ninstall.packages(\"devtools\")\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\")\ndevtools::install_github(\"nuriamw/micro4all\")\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ShortRead\")\ninstall.packages(\"tidyverse\")\n\nlibrary(devtools)\nlibrary(dada2)\nlibrary(micro4all)\nlibrary(ShortRead)\nlibrary(tidyverse)\n```\n\nAs you can see, we are going to use different functions of [DADA2 package](https://benjjneb.github.io/dada2/tutorial.html \"Click to DADA2 website\"), so it will be nice if you read all the documentation regarding this package.\n\n::: callout-important\nThe steps 1 to 7 must be performed for each sequencing run. If you are working with different sequencing runs, you have to join all the sequences tables into one object (step 8). Once the joined element is obtained, steps 9-last step have to be applied to the joined object.\n:::\n\n::: callout-warning\nIf you have included a Mock community which does not include fungal sequences (or a very small number of fungal sequences) in your sequencing run, I suggest you to process first the *16S rRNA* dataset.\n:::\n\n# 1. Formatting the name of the samples\n\nNow, we will start! Set the working directory and specify the path where you will be working:\n\n```{r}\npath= \"~/Platano_PLEC/PLEC_muestreo2022/310523_ITS/reads\"\nlist.files(path) #Check that all files are here included (script + fastq files)\n```\n\nSort F and R reads separately and save them into two variables:\n\n```{r}\nfnFs = sort(list.files(path, pattern=\"_R1_001.fastq.gz\", full.names = TRUE))\nfnRs = sort(list.files(path, pattern=\"_R2_001.fastq.gz\", full.names = TRUE))\n#our filenames have the format \"NGS015-23-ITS2-A2S12R_S51_L001_R2_001.fastq.gz\"\n```\n\nExtract the name (code) of our samples, and remove all the extra information from the filenames:\n\n```{r}\nsample.names_raw = sapply(strsplit(basename(fnFs), \"_\"), `[`, 1) \n#split the string when \"_\" is found and keep the 4th part of the string\n#we will get, for example, this: NGS015-23-ITS2-A2S12R\n\nsample.names= gsub(patter = \"NGS015-23-ITS2-\", replacement = \"\", sample.names)#here we replace the extra information by nothing (no characters)\nsample.names\n```\n\n::: callout-note\nNote that in this case, we have modified the sample names by employing a different way from that used for bacterial dataset. However, the result is exactly the same.\n:::\n\n# 2. Check the quality of the sequencing\n\nThere are different ways to check the quality of the reads:\n\n## a) Count the number of reads\n\nIt would be nice to check whether we obtained enough reads from the sequencing service. Otherwise, we should ask for the service to repeat the sequencing.\n\n```{r}\nraw_reads_count = NULL\n\nfor (i in 1:length(fnFs)){\n    raw_reads_count = rbind(raw_reads_count, \n                            length(ShortRead::readFastq(fnFs[i])))\n  } #this loop counts the number of F reads by means of the ShortRead package\n\nrownames(raw_reads_count)= sample.names #formatting of the output\ncolnames(raw_reads_count)= \"Number_of_reads\"\na=data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count)\n\nraw_reads_count2 = NULL\nfor (i in 1:length(fnRs)){#do the same with R reads\n  raw_reads_count2 <- rbind(raw_reads_count2, \n                            length(ShortRead::readFastq(fnRs[i])))\n} \nrownames(raw_reads_count2)= sample.names\ncolnames(raw_reads_count2)= \"Number_of_reads\"\nb=data.frame(\"_\"=rownames(raw_reads_count2),raw_reads_count2)\n\na==b #check that we get the same number of F and R reads\n\ncbind(row.names(raw_reads_count)[which.min(raw_reads_count)],\n      min(raw_reads_count))#check which sample accounts for the lowest number of reads. Be careful if you keep the negative control of the sequencing\ncbind(row.names(raw_reads_count)[which.max(raw_reads_count)],\n      max(raw_reads_count))#check which sample accounts for the highest number of reads.\n\nwrite.table(data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count),\n            file=\"Num_raw_reads_ITS.txt\", sep=\"\\t\",row.names =F)\nView(raw_reads_count) #please, check the number of reads per sample\n```\n\nAt this point, we have to check the number of raw reads per sample. The researcher has to verify whether this number is enough or not. It depends on the type of sample, among other parameters.\n\n## b) Inspect the length of the reads\n\n::: callout-important\nRemember that ITS2 varies in length among different fungi. We have to keep this in mind along the analysis, since we have to skip several steps that are based on the length of the reads (comparing to the processing of *16S rRNA* reads).\n:::\n\nIn spite of that indicated in the above callout, we will inspect the length of the reads just to confirm the genomics service has performed a good job.\n\nIn our case, the genomics service followed a 2x300 bp PE strategy.\n\n```{r}\nreads=ShortRead::readFastq(fnFs) #save the reads in a new variable with the format of ShortRead package, which is a bit different from standard R variable\nuniques = unique(reads@quality@quality@ranges@width) #get the length of the reads\n\ncounts= NULL\n\nfor (i in 1:length(uniques)) {\n  counts=rbind(counts,\n               length(which(reads@quality@quality@ranges@width==uniques[i])))\n  \n}#specific loop to count the number of reads of each length.  \n\nhistogram =  cbind(uniques,counts)\ncolnames(histogram) = c(\"Seq.length\", \"counts\")\n\n#check the histogram\nhead(histogram[order(histogram[,1],decreasing = TRUE),]) #Most of the sequences should fall in expected sequence length\n\n#plotting\nhist(reads@quality@quality@ranges@width, main=\"Forward length distribution\",\n     xlab=\"Sequence length\", ylab=\"Raw reads\")\n\nwrite.table(histogram, file=\"Lenght_raw_reads_ITS.txt\", sep=\"\\t\",row.names =F)\n```\n\n## c) Check the quality plots\n\nCheck if the overlapping of F and R reads is possible considering the quality of the last nucleotides. Be careful with the quality of R reads (which tends to be worse than that of F reads, especially in the latest part of the reads).\n\n```{r}\nplotQualityProfile(fnFs[4:5])#select the specific samples you want to check, in this case 4 and 5\nplotQualityProfile(fnRs[4:5])\n```\n\n# 3. Filter and trimming step\n\n::: callout-note\nIf you come from the processing of bacterial reads, you would have noticed that here we have skipped the step of FIGARO. That tool needs that all the reads are of the same length, so we cannot use it for fungal (ITS2) dataset.\n:::\n\n```{r}\nfiltFs=file.path(path, \"filtered\", basename(fnFs))#create the \"filtered\" directory\nfiltRs=file.path(path, \"filtered\", basename(fnRs))\n```\n\nSince we cannot use FIGARO, we have to test different parameters in the following filtering and trimming step. It is recommended to achieve a compromise between the percentage of retained sequences and their quality. So, run the following command indicating specific values of maxEE according to the quality plots of F and R reads previously visualized in the **step 2.c)**. Write down the results obtained for that maxEE, and then re-run with another maxEE values, until you reach to the best solution.\n\n```{r}\nout=filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN=0, maxEE=c(3,4), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE, minLen=50)\n\n\n#maxN=0 : remove the reads with at least 1 ambiguity\n#trunQ=2: remove all the reads with one nucleotide with 2 or less quality value\n#maxEE: maximum expected error for F and R reads\n#minLen=50: remove reads shorter than 50 bp\n\nhead(out)#important! Check the results\n```\n\nIn our case, we selected:\n\n| Sequencing run | Positions |       | maxEE |       |\n|----------------|-----------|-------|-------|-------|\n|                | **F**     | **R** | **F** | **R** |\n| **Run5**       |           |       | 3     | 4     |\n| **Run6**       |           |       | 3     | 4     |\n| **Run7**       |           |       | 4     | 6     |\n\n::: callout-note\nIf you come from the processing of bacterial reads, you would have noticed that in the above code we have not specified the trimming positions. This is not a mistake; we do not indicate the trimming position since the ITS2 region is variable in length and we will keep all the sequences regardless of their length.\n:::\n\nOur reads are partially filtered and trimmed (still have the primers!).\n\n# 4. Cutadapt: removal of the primers\n\nOur reads still have the primers (which are artificial sequences and can have ambiguities). So, we have to remove them and discard all the sequences in which primers are not found (because if the sequencing run was ok, primers should had been found inside the reads). Let's use cutadapt tool, which does not run in R but in python:\n\n```{r}\nFWD = \"GTGARTCATCGAATCTTTG\" #sequence of F primer\nREV = \"TCCTCCGCTTATTGATATGC\" #sequence of R primer\n```\n\n```{r}\nallOrients = function(primer) {\n  require(Biostrings)\n  dna =DNAString(primer)  #package Biostrings does not work with characters but with strings, so let's change the class of the primers variables.\n  orients = c(Forward = dna, Complement = Biostrings::complement(dna), \n              Reverse = reverse(dna), RevComp = reverseComplement(dna))\n  return(sapply(orients, toString)) #change from string to character\n} #this function calculates all the possible orientations of the primers\n\nFWD.orients = allOrients(FWD) #pass the function to F and R primers\nREV.orients  = allOrients(REV)\nFWD.orients\nREV.orients\n```\n\n```{r}\nprimerHits =function(primer, fn) {\n  nhits =vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n  return(sum(nhits > 0))\n} #it counts the number of sequence in each orientation\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = filtFs[[4]]),#check for instante, in sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = filtRs[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = filtFs[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = filtRs[[4]]))\n#here we make a summary table. Please, visit DADA2 website for a better interpretation of the table.\n```\n\nNow yes, we are running cutadapt, although we have to adapt the reads previously for cutadapt. This tool need some flags that have to be added to the reads. Visit the web of [cutadapt](https://cutadapt.readthedocs.io/en/stable/guide.html \"User guide of cutadapt\").\n\n```{r}\ncutadapt  =  \"/usr/bin/cutadapt\" #path to cutadapt \n\nsystem2(cutadapt, args = c(\"--version\")) # Run shell commands from R\n\npath.cut =file.path(path, \"cutadapt\") #create a directory where processed reads will be saved\n\nif(!dir.exists(path.cut)) dir.create(path.cut)\n\nfnFs.cut =file.path(path.cut, basename(filtFs))#if you came from the processing of 16S rRNA reads, be careful here. We have to use \"filtFs\" instead of \"fnFs\" because we have already performed the filtering and trimming steps.\nfnRs.cut =file.path(path.cut, basename(filtRs))#the same comment here\n\n#Produce arguments for cutadapt\nFWD.RC = dada2:::rc(FWD)  \nREV.RC = dada2:::rc(REV)\n\nR1.flags  =  paste0(\"-a\", \" \", \"^\",FWD,\"...\", REV.RC) #adding the flags\nR2.flags  =  paste0(\"-A\",\" \",\"^\", REV, \"...\", FWD.RC)\n\n#run cutadapt\nfor(i in seq_along(fnFs)) {\n  system2(cutadapt, args = c(R1.flags, R2.flags, \"-n\", 2,\"-m\", 1, \"--discard-untrimmed\", \"-j\",0, \"-o\", fnFs.cut[i], \"-p\", fnRs.cut[i], filtFs[i], filtRs[i],\"--report=minimal\")) \n}\n\n#-n 2: remove the primers\n#-m 1: remove empty sequences\n#-j 0: automatically detect number of cores\n#-0: output files\n#-i: input files. We use the filtered and trimmed reads\n```\n\nNow, we check whether cutadapt has removed all the primers. For that purpose, we pass the previously created function to check the number of times each primer appears in our dataset:\n\n```{r}\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[4]]),#Here we can indicate the number of the sample we want to check. In this case, sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[4]]))\n```\n\n::: callout-note\nNote that in some cases, after the removal of the primers by cutadapt, the function *primerHits* still finds some primers in our dataset. Do no panic, these differences are normal because cutadapt and that DADA2 functions are based on different algorithms.\n:::\n\n# 5. DADA2: machine learning\n\nWe are going to use DADA2 to denoise, infer the samples and to correct the expected errors by applying different functions of the package.\n\n```{r}\n# First, learn error rates\nerrF =learnErrors(fnFs.cut, multithread=T, verbose=1)\nerrR =learnErrors(fnRs.cut, multithread=T, verbose=1)\n\n# Lets plot the errors in each position\nplotErrors(errF, nominalQ=TRUE)\nplotErrors(errR, nominalQ=TRUE)\n#these plots show the possible errors made for each possible mutation (A>C, A>G, etc). Dots should be close to the red line.\n```\n\n# 6. DADA2: Sample inference\n\n```{r}\ndadaFs = dada(fnFs.cut, err=errF, multithread=TRUE)\ndadaRs = dada(fnRs.cut, err=errR, multithread=TRUE)\ndadaFs[[4]]#check the inferred fourth sample (it's just an example). We will obtain the number of true sequence variants from the X number of unique sequences.\n\n# Set sample names\nnames(dadaFs) = sample.names\nnames(dadaRs) = sample.names\n```\n\n# 7. Overlap F and R sequences\n\nIn this step we are going to overlap F (forward) and R (reverse) reads coming from the same sample.\n\n```{r}\nmergers=mergePairs(dadaFs, fnFs.cut, dadaRs, fnRs.cut, verbose=TRUE)\nhead(mergers[[4]]) #check the output for sample number 4\n```\n\nNow, we are going to construct an amplicon sequence variant table (ASV table).\n\nBe careful, because we will get one ASV table per sequencing run. Thus, as we got 3 runs for fungal amplicons, we will get 3 different ASV tables.\n\n```{r}\nseqtab_run5= makeSequenceTable(mergers)\ndim(seqtab_run5) #indicates the number of samples (including negative controls) and the number of ASVs\nsaveRDS(seqtab_run5, \"~/Platano_PLEC/PLEC_muestreo2022/310523_ITS/reads/seqtab_run5.rds\") #save the seqtab in .rds format\n```\n\n# 8. Merge sequence tables from different runs\n\nAs indicated before, in this step we are going to merge the sequence tables (\"seqtabs\") coming from different runs. So, firstly, we have to load all of them to the current project. Please, note that all the seqtabs have to be in the same working directory!\n\n```{r}\nseqtab_run5=readRDS(\"seqtab_run5.rds\")#load all the .rds files in the same directory\nseqtab_run6=readRDS(\"seqtab_run6rds\")\nseqtab_run7=readRDS(\"seqtab_run7rds\")\n\n\nmergedSeqTab=mergeSequenceTables(seqtab_run5,seqtab_run6,seqtab_run7,repeats=\"sum\")\n#with \"sum\" we indicate that we want to join and sum all the reads that are in samples with the same name. Why do we do that? \n```\n\n# 9. Chimeral removal\n\nAs the sequencing process is based on PCRs, chimeric sequences are expect to appear. So, let's remove them.\n\n```{r}\nseqtab.nochim = removeBimeraDenovo(mergedSeqTab, method=\"consensus\", multithread=TRUE, verbose=TRUE)#if you just are working on one dataset (coming from just one sequencing run, instead of \"mergedSeqTab\", you have to write \"seqt_tab\" or the name of your seqtab)\ndim(seqtab.nochim) #indicates the number of samples and ASVs (but not the number of sequences)\n```\n\n# 10. Other steps: flter the ASVs by their length\n\nLet's check the number of ASVs and their length:\n\n```{r}\ntable(nchar(getSequences(seqtab.nochim)))  #Number of ASV of each length\n  #first line: length of the sequences\n  #second line: number of sequences of each length\n```\n\nCalculate the number of sequences of each ASV that of each corresponding length:\n\n```{r}\nreads.per.seqlen = tapply(colSums(seqtab.nochim), factor(nchar(getSequences(seqtab.nochim))), sum)\nreads.per.seqlen\n\ntable_reads_seqlen = data.frame(length=as.numeric(names(reads.per.seqlen)), count=reads.per.seqlen)\nggplot(data=table_reads_seqlen, aes(x=length, y=count)) + geom_col()\n```\n\n# 11. Taxonomic classification\n\nWe are going to classify our ITS2 amplicons against [UNITE](https://doi.org/10.15156/BIO/2938067 \"Click to the paper\") v.9.0 database, which is host in our PC or server.\n\n```{r}\ntaxa_unite =assignTaxonomy(seqtab.nochim, \"/home/databases/sh_general_release_dynamic_25.07.2023.fasta\", multithread=TRUE)#indicate the path were your database is located\n\nASV = seqtab.nochim #edit the tables\nASVt = t(ASV)\n\ntaxa_na = apply(taxa_unite,2, tidyr::replace_na, \"unclassified\")[,-7] #replace the \"NA\" values by \"unclassified\" in those case in which an ASV have not been identified at a specific taxonomic rank\ntaxa_rdp_na=taxa_rdp_na[,-(7:15)]\n\n#we are going to modify a bit the tables so that the name of ASVs looks like \"ASV00001\" (in case we have 10000 ASVs or more). In case we have just 100 ASVs, they will have that code: \"ASV001\"\n\nnumber.digit = nchar(as.integer(nrow(ASVt)))\nnames =paste0(\"ASV%0\", number.digit, \"d\") #As many 0 as digits\nASV_names<- sprintf(names, 1:nrow(ASVt))\n\nASV_table_classified_raw = cbind(as.data.frame(taxa_na,stringsAsFactors = FALSE),as.data.frame(ASV_names, stringsAsFactors = FALSE),as.data.frame(ASVt,stringsAsFactors = FALSE))\n\nASV_seqs = rownames(ASV_table_classified_raw)\nrownames(ASV_table_classified_raw) <- NULL\nASV_table_classified_raw = cbind(ASV_seqs, ASV_table_classified_raw)\n\nwrite.table(ASV_table_classified_raw, file=\"ASV_table_classified_raw_wirhMockwithPlastids.txt\", sep=\"\\t\")\n```\n\nNow, we have our ASV table with the corresponding taxonomic classification. But, it still potentially includes some sequences that could be erroneous.\n\n# 12. MOCK Community: setting the sequencing detection limit\n\nThe Mock Community we used in our sequencing runs just includes sequences of two fungi (yeast indeed). Thus, we will consider that the detection limit previously established for bacterial dataset is the same as for fungi. Ours was **0.001207%** of the sequences. Let's use it:\n\n```{r}\nASV_sums=rowSums(ASV_table_classified_raw[,9:ncol(ASV_table_classified_raw)])\nsum.total=sum(ASV_sums)# Get the total number of sequences\nnseq_cutoff=(0.001207/100)*sum.total# Apply the percentage to sequence number\n\n# Now, filter the table accordingly:\nASV_filtered=ASV_table_classified_raw[which(ASV_sums>nseq_cutoff),] #we are retaining just those ASVs accounting for more sequences that that determined by the cut-off\nASV_table=ASV_filtered[order(ASV_filtered[[\"ASV_names\"]]),]# Sort table in ascending order of ASV names\n```\n\n# 13. Removal of erronous sequences and taxonomy refinement\n\nLet's remove all the undesired sequences (if we are working with plant endophytes, we will have retained plant sequences, e.g., chloroplasts). We will also remove those ASVs not classified even at kingdom level.\n\n```{r}\nASV_final=ASV_table[(which(ASV_table$Kingdom!=\"unclassified\")),] #remove all the sequences not classified at Kingdom level\n```\n\nSometimes, when we have worked with plant tissues, we can find some fungi that are ascribed to \"*incertae sedis*\" group (missclassified) even at phylum level. They tend to be sequences of the plant host. So, let's verify it and remove them in case we find these sequences:\n\n```{r}\nincertae_phy=ASV_final[which(ASV_final$Phylum==\"p__Fungi_phy_Incertae_sedis\"),] #UNITE database writes \"p__\" prior to the name of the phyla\nseq_incertae=as.list(incertae_phy$ASV_seqs)\nwrite.fasta(seq_incertae, names=incertae_phy$ASV_names, file.out=\"incertaesedis_phylum.fas\", open = \"w\", nbchar =1000 , as.string = FALSE)#save the fasta file of these ASVs\n\n#Let's perform a comparison of these sequences against the NCBI GenBank database by BLASTn\nsystem((\"/home/programas/ncbi-blast-2.13.0+/bin/blastn -query incertaesedis_phylum.fas -db /home/databases/nt -out incertae_phylum_hits.txt -outfmt '6 std stitle' -show_gis -max_target_seqs 20 -parse_deflines -num_threads 10\"),intern = TRUE)\n\n#The output of the BLAST (\"incertae_phylum_hits.txt\") have to be manually checked.\n\n#In our case, all the sequences are related to *Musa* spp. and other eukaryots that are not fungi, so we are going to remove all these sequences:\n\nASV_final2 = ASV_final[(which(ASV_final$Phylum!=\"p__Fungi_phy_Incertae_sedis\")),]\n```\n\nRepeat the same with the unclassified sequences at phylum level:\n\n```{r}\nunclassified_phy=ASV_final2[which(ASV_final2$Phylum==\"unclassified\"),]\nseq_unclassified= as.list(unclassified_phy$ASV_seqs)\nwrite.fasta(seq_unclassified, names=unclassified_phy$ASV_names, file.out=\"unclassified_phylum.fas\", open = \"w\", nbchar =1000 , as.string = FALSE)\nsystem((\"/home/programas/ncbi-blast-2.13.0+/bin/blastn -query unclassified_phylum.fas -db /home/databases/nt -out unclassified_phylum_hits.txt -outfmt '6 std stitle' -show_gis -max_target_seqs 5 -parse_deflines -num_threads 10\"),intern = TRUE)\n\n#We found a lot of missclassified ASVs at phylum, so we are going to remove them especifically:\n\nASV_final3 = ASV_final2[(which(ASV_final2$ASV_names!=\"ASV0004\"&\n                                ASV_final2$ASV_names!=\"ASV0012\"&\n                                ASV_final2$ASV_names!=\"ASV0109\"&\n                                ASV_final2$ASV_names!=\"ASV0140\"&\n                                ASV_final2$ASV_names!=\"ASV0269\"&\n                                ASV_final2$ASV_names!=\"ASV0627\"&\n                                ASV_final2$ASV_names!=\"ASV0654\"&\n                                ASV_final2$ASV_names!=\"ASV1112\"&\n                                ASV_final2$ASV_names!=\"ASV0938\"&\n                                  ASV_final2$ASV_names!=\"ASV0911\"&\n                                  ASV_final2$ASV_names!=\"ASV0731\"&\n                                  ASV_final2$ASV_names!=\"ASV0844\"&\n                                  ASV_final2$ASV_names!=\"ASV0923\"&\n                                  ASV_final2$ASV_names!=\"ASV1080\"&\n                                 ASV_final2$ASV_names!=\"ASV1096\"&\n                                 ASV_final2$ASV_names!=\"ASV1147\"&\n                                 ASV_final2$ASV_names!=\"ASV1291\"&\n                                 ASV_final2$ASV_names!=\"ASV1321\"&\n                                 ASV_final2$ASV_names!=\"ASV1335\")),]\n\n```\n\nEventually, save the definitive ASV table:\n\n```{r}\nwrite.table(data.frame(\" \"=rownames(ASV_final3),ASV_final3),file=\"ASV_Hongos_FINAL.txt\", sep=\"\\t\",row.names =F)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Fungi_reads_processing.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"minty","title":"Processing of ITS2 reads obtained from Illumina MiSeq platform","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}