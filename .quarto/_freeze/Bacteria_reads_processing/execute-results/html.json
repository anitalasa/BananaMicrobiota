{
  "hash": "c000a76c75df7c09f89b9bcca6cea029",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Processing of *16S rRNA* reads obtained from Illumina MiSeq platform\"\nformat:\n  html:\n    toc: true\nexecute:\n  eval: false\neditor_options: \n  chunk_output_type: console\n---\n\n\nFirst of all, we will install and load of the required packages and libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\")\ndevtools::install_github(\"nuriamw/micro4all\")\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ShortRead\")\ninstall.packages(\"tidyverse\")\n\nlibrary(devtools)\nlibrary(dada2)\nlibrary(micro4all)\nlibrary(ShortRead)\nlibrary(tidyverse)\n```\n:::\n\n\nAs you can see, we are going to use different functions of [DADA2 package](https://benjjneb.github.io/dada2/tutorial.html \"Click to DADA2 website\"), so it will be nice if you read all the documentation regarding this package.\n\n::: callout-important\nThe steps 1 to 8 must be performed for each sequencing run. If you are working with different sequencing runs, you have to join all the sequences tables into one object (step 9). Once the joined element is obtained, steps 10-last step have to be applied to the joined object.\n:::\n\n# 1. Formatting the name of the samples\n\nNow, we will start! Set the working directory and specify the path where you will be working:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath= \"~/Platano_PLEC/PLEC_muestreo2022/BACTERIA/reads\"#insert here the path where your fastq files are\nlist.files(path) #Check that all files are here included (script + fastq files)\n```\n:::\n\n\nSort F and R reads separately and save them into two variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfnFs = sort(list.files(path, pattern=\"_R1_001.fastq.gz\", full.names = TRUE))\nfnRs = sort(list.files(path, pattern=\"_R2_001.fastq.gz\", full.names = TRUE))\n#our filenames have the format \"NGS015-23-16S-A2S12R_S51_L001_R2_001.fastq.gz\"\n```\n:::\n\n\nExtract the name (code) of your samples, and remove all the extra information from the filenames:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample.names_raw = sapply(strsplit(basename(fnFs), \"-\"), `[`, 4) \n#split the string when \"-\" is found and keep the 4th part of the string\n#we will get, for example, this: \"A2S12R_S51_L001_R2_001.fastq.gz\"\"\nsample.names = sapply(strsplit(basename(sample.names_raw), \"_\"), `[`, 1)\n#removal of extra information (in this example, split sample.names_raw when \"_\" is found, and get the first part of the string)\n```\n:::\n\n\nWe are going to rename the samples of the mock community, so that they satisfy the requirement of the functions to be applied in next steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample.names=gsub(\"MOCK1\", \"MOCK-1\", sample.names)#replace \"MOCK1\" by \"MOCK-1\"\nsample.names=gsub(\"MOCK2\", \"MOCK-2\", sample.names)\nsample.names=gsub(\"MOCK3\", \"MOCK-3\", sample.names)\nsample.names #check that the name of our samples is OK\n```\n:::\n\n\n# 2. Check the quality of the sequencing\n\nThere are different ways to check the quality of the reads:\n\n## a) Count the number of reads\n\nIt would be nice to check whether we obtained enough reads from the sequencing service. Otherwise, we should ask the service to to repeat the sequencing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_reads_count = NULL\n\nfor (i in 1:length(fnFs)){\n    raw_reads_count = rbind(raw_reads_count, \n                            length(ShortRead::readFastq(fnFs[i])))\n  } #this loop counts the number of F reads by means of the ShortRead package\n\nrownames(raw_reads_count)= sample.names #formatting of the output\ncolnames(raw_reads_count)= \"Number_of_reads\"\na=data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count)\n\nraw_reads_count2 = NULL\nfor (i in 1:length(fnRs)){#do the same with R reads\n  raw_reads_count2 <- rbind(raw_reads_count2, \n                            length(ShortRead::readFastq(fnRs[i])))\n} \nrownames(raw_reads_count2)= sample.names\ncolnames(raw_reads_count2)= \"Number_of_reads\"\nb=data.frame(\"_\"=rownames(raw_reads_count2),raw_reads_count2)\n\na==b #check that we get the same number of F and R reads\n\ncbind(row.names(raw_reads_count)[which.min(raw_reads_count)],\n      min(raw_reads_count))#check which sample accounts for the lowest number of reads. Be carefual if you keep the negative control of the sequencing\ncbind(row.names(raw_reads_count)[which.max(raw_reads_count)],\n      max(raw_reads_count))#check which sample accounts for the highest number of reads.\n\nwrite.table(data.frame(\"_\"=rownames(raw_reads_count),raw_reads_count),\n            file=\"Num_raw_reads_16s.txt\", sep=\"\\t\",row.names =F)\nView(raw_reads_count) #please, check the number of reads per sample\n```\n:::\n\n\nAt this point, we have to check the number of raw reads per sample. The researcher has to verify whether this number is enough or not. It depends on the type of sample, among other parameters.\n\n## b) Inspect the length of the reads\n\nIn our case, the genomics service followed 2x275 bp and 2x300 bp PE strategy, so most of the sequences are expected to have 275 or 300 bp.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreads=ShortRead::readFastq(fnFs) #save the reads in a new variable with the format of ShortRead package, which is a bit different from standard R variable\nuniques = unique(reads@quality@quality@ranges@width) #get the length of the reads\n\ncounts= NULL\n\nfor (i in 1:length(uniques)) {\n  counts=rbind(counts,\n               length(which(reads@quality@quality@ranges@width==uniques[i])))\n  \n}#specific loop to count the number of reads of each length.  \n\nhistogram =  cbind(uniques,counts)\ncolnames(histogram) = c(\"Seq.length\", \"counts\")\n\n#check the histogram\nhead(histogram[order(histogram[,1],decreasing = TRUE),]) #Most of the sequences should fall in expected sequence length\n\n#plotting\nhist(reads@quality@quality@ranges@width, main=\"Forward length distribution\",\n     xlab=\"Sequence length\", ylab=\"Raw reads\")\n\nwrite.table(histogram, file=\"Lenght_raw_reads_16S.txt\", sep=\"\\t\",row.names =F)\n```\n:::\n\n\n## c) Check the quality plots\n\nIn this step, keep in mind the expected length of your amplicon. Then, check if the overlapping of F and R reads is possible considering the quality of the last nucleotides.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotQualityProfile(fnFs[4:5])#select the specific samples you want to check, in this case 4 and 5\nplotQualityProfile(fnRs[4:5])\n```\n:::\n\n\n# 3. FIGARO tool\n\nWe will use FIGARO to determine the best position of trimming in both F and R reads, and the best maximum expected errors for DADA2. WARNING: FIGARO does not run in R (not even in Windows!), so we will run it in python3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigFs = file.path(path, \"figaro\", basename(fnFs)) #create the directory where the output will be saved\nfigRs = file.path(path, \"figaro\", basename(fnRs))\n```\n:::\n\n\nFIGARO does not work if all the samples are of different lengths. Thus, we have to cut the sequences so that this tool works. Do not panic because this cut is just made in this step. Then, we will work with the full-length reads\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout.figaro=filterAndTrim(fnFs, figFs, fnRs, figRs,compress=TRUE, \n                         multithread=TRUE, truncLen=c(271,271)) \n#select the position so that most of the sequences are considered \n#(check the histogram or the dataframe with the lengths of the reads to select this \"pseudo\"trimming positions)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfigaro=system((\"python3 /home/programas/figaro/figaro/figaro.py  -i ~/Platano_PLEC/PLEC_muestreo2022/BACTERIA/reads/figaro -o ~/Platano_PLEC/PLEC_muestreo2022/BACTERIA/reads/figaro -a 426 -f 17 -r 21\"), intern=TRUE) \n\n#indicate the path where the script \"figaro.py\" is located\n#-a, indicate the length of the amplicon, \n#f, indicate the length of primer F\n#r, indicate the length R  primer\n\nhead(figaro) #select the best parameters proposed by FIGARO, here: Trimming position F,R: 248,236; maximum expected error F, R: 3,3.\n```\n:::\n\n\n# 4. Filter and trimming step\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiltFs=file.path(path, \"filtered\", basename(fnFs))#create the \"filtered\" directory\nfiltRs=file.path(path, \"filtered\", basename(fnRs))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nout=filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(248,236),\n                     maxN=0, maxEE=c(3,3), truncQ=2, rm.phix=TRUE,\n                     compress=TRUE, multithread=TRUE, minLen=50)\n\n#truncLen: introduce the best trimming position proposed by FIGARO\n#maxN=0 : remove the reads with at least 1 ambiguity\n#trunQ=2: remove all the reads with one nucleotide with 2 or less quality value\n#maxEE: maximum expected error for F and R reads proposed by FIGARO\n#minLen=50: remove reads shorter than 50 bp\n\nhead(out)\n```\n:::\n\n\nOur reads are partially filtered and trimmed (still have the primers!)\n\nHere you have the parameters we obtained for each sequencing runs:\n\n| Run      | Positions |       | maxEE |       |\n|----------|-----------|-------|-------|-------|\n|          | **F**     | **R** | **F** | **R** |\n| **Run1** | 248       | 236   | 3     | 3     |\n| **Run2** | 272       | 212   | 2     | 2     |\n| **Run3** | 278       | 206   | 2     | 2     |\n| **Run4** | 273       | 211   | 2     | 2     |\n\n# 5. Cutadapt: removal of the primers\n\nOur reads still have the primers (which are artificial sequences and can have ambiguities). So, we have to remove them and discard all the sequences in which primers are not found (because if the sequencing run was ok, primers should had been found inside the reads). Lets use cutadapt tool, which does not run in R but in python:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFWD=c(\"CCTACGGGNBGCASCAG\") #insert the sequences of the primers\nREV=c(\"GACTACNVGGGTATCTAATCC\") #here we have 2 degenerations (ambiguities)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nallOrients = function(primer) {\n  require(Biostrings)\n  dna =DNAString(primer)  #package Biostrings does not work with characters but with strings, so let's change the class of the primers variables.\n  orients = c(Forward = dna, Complement = Biostrings::complement(dna), \n              Reverse = reverse(dna), RevComp = reverseComplement(dna))\n  return(sapply(orients, toString)) #change from string to character\n} #this function calculates all the possible orientations of the primers\n\nFWD.orients = allOrients(FWD) #pass the function to F and R primers\nREV.orients  = allOrients(REV)\nFWD.orients\nREV.orients\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprimerHits =function(primer, fn) {\n  nhits =vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n  return(sum(nhits > 0))\n} #it counts the number of sequence in each orientation\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = filtFs[[4]]),#check for instante, in sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = filtRs[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = filtFs[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = filtRs[[4]]))\n#here we make a summary table. Please, visit DADA2 website for a better interpretation of the table.\n```\n:::\n\n\nNow yes, we are running cutadapt, although we have to adapt the reads previously for cutadapt. This tool need some flags that have to be added to the reads. Visit the web of [cutadapt](https://cutadapt.readthedocs.io/en/stable/guide.html \"User guide of cutadapt\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutadapt  =  \"/usr/bin/cutadapt\" #path to cutadapt \n\nsystem2(cutadapt, args = c(\"--version\")) # Run shell commands from R\n\npath.cut =file.path(path, \"cutadapt\") #create a directory where processed reads will be saved\n\nif(!dir.exists(path.cut)) dir.create(path.cut)\n\nfnFs.cut =file.path(path.cut, basename(fnFs))\nfnRs.cut =file.path(path.cut, basename(fnRs))\n\n#Produce arguments for cutadapt\nFWD.RC = dada2:::rc(FWD)  \nREV.RC = dada2:::rc(REV)\n\nR1.flags  =  paste0(\"-a\", \" \", \"^\",FWD,\"...\", REV.RC) #adding the flags\nR2.flags  =  paste0(\"-A\",\" \",\"^\", REV, \"...\", FWD.RC)\n\n#run cutadapt\nfor(i in seq_along(fnFs)) {\n  system2(cutadapt, args = c(R1.flags, R2.flags, \"-n\", 2,\"-m\", 1, \"--discard-untrimmed\", \"-j\",0, \"-o\", fnFs.cut[i], \"-p\", fnRs.cut[i], filtFs[i], filtRs[i],\"--report=minimal\")) \n}\n\n#-n 2: remove the primers\n#-m 1: remove empty sequences\n#-j 0: automatically detect number of cores\n#-0: output files\n#-i: input files. We use the filtered and trimmed reads\n```\n:::\n\n\nNow, we will check whether cutadapt has removed all the primers. For that purpose, we pass the previously created function to check the number of times each primer appears in our dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[4]]),#Here we can indicate the number of the sample we want to check. In this case, sample number 4\n      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[4]]),\n      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[4]]),\n      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[4]]))\n```\n:::\n\n\n::: callout-note\nNote that in some cases, after the removal of the primers by cutadapt, the function primerHits still finds some primers in our dataset. Do no pannic; these differences are normal because cutadapt and that DADA2 functions are based on different algorithms.\n:::\n\n# 6. DADA2: machine learning\n\nWe are going to use DADA2 to denoise, infer the samples and to correct the expected errors by applying different functions of the package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, learn error rates\nerrF =learnErrors(fnFs.cut, multithread=T, verbose=1)\nerrR =learnErrors(fnRs.cut, multithread=T, verbose=1)\n\n# Lets plot the errors in each position\nplotErrors(errF, nominalQ=TRUE)\nplotErrors(errR, nominalQ=TRUE)\n#these plots show the possible errors made for each possible mutation (A>C, A>G, etc). Dots should be close to the red line.\n```\n:::\n\n\n# 7. DADA2: Sample inference\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndadaFs = dada(fnFs.cut, err=errF, multithread=TRUE)\ndadaRs = dada(fnRs.cut, err=errR, multithread=TRUE)\ndadaFs[[4]]#check the inferred fourth sample (it's just an example). We will obtain the number of true sequence variants from the X number of unique sequences.\n\n# Set sample names\nnames(dadaFs) = sample.names\nnames(dadaRs) = sample.names\n```\n:::\n\n\n# 8. Merge sequences\n\nIn this step we are going to overlap F and R reads coming from the sample sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmergers=mergePairs(dadaFs, fnFs.cut, dadaRs, fnRs.cut, verbose=TRUE)\nhead(mergers[[4]]) #check the output for sample number 4\n```\n:::\n\n\nNow, we are going to construct an amplicon sequence variant table (ASV table).\n\nBe careful, because we will get one ASV table per sequencing run. Thus, as we got 4 runs for bacterial amplicons, we will get 4 different ASV table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab_run1= makeSequenceTable(mergers)\ndim(seqtab_run1) #indicates the number of samples (including mock community and negative controls) and the number of ASVs\nsaveRDS(seqtab_run1, \"~/Platano_PLEC/PLEC_muestreo2022/BACTERIA/reads/seqtab_run1.rds\") #save the seqtab in .rds format\n```\n:::\n\n\n# 9. Merge sequence tables from different runs\n\nAs indicated before, in this step we are going to merge the sequence tables coming from different runs. So, firstly, we have to load all of them to the current project. Please, note that all the seqtabs have to be in the same working directory!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab_run1=readRDS(\"seqtab_run1.rds\")#load all the .rds files in the same directory\nseqtab_run2=readRDS(\"seqtab_run2.rds\")\nseqtab_run3=readRDS(\"seqtab_run3.rds\")\nseqtab_run4=readRDS(\"seqtab_run4.rds\")\n\nmergedSeqTab=mergeSequenceTables(seqtab_run1,seqtab_run2,seqtab_run3,seqtab_run4,\n                                 repeats=\"sum\")\n#with \"sum\" we indicate that we want to join and sum all the reads that are in samples with the same name. Why do we do that? Because in each sequencing run we have 3 replicates of the mock community, and we are going to sum the same replicates from different runs.\n#e.g., MOCK-1 (run1) + MOCK-1 (run2)+ MOCK-1 (run3)+ MOCK-1 (run4)\n```\n:::\n\n\n# 10. Chimeral removal\n\nAs the sequencing process is based on PCRs, chimeric sequences are expect to appear. So, let's remove them\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.nochim = removeBimeraDenovo(mergedSeqTab, method=\"consensus\", multithread=TRUE, verbose=TRUE)\ndim(seqtab.nochim) #indicates the number of samples and ASVs (but not the number of sequences)\n```\n:::\n\n\n# 11. Other steps: flter the ASVs by their length\n\nLet's check the number of ASVs and their length\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nchar(getSequences(seqtab.nochim)))  #Number of ASV of each length\n  #first line: length of the sequences\n  #second line: number of sequences of each length\n\n#Our amplicon is of 426 bp in length, so most of the sequences would have that length\n```\n:::\n\n\nCalculate the number of sequences of each ASV that that each length:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreads.per.seqlen = tapply(colSums(seqtab.nochim), factor(nchar(getSequences(seqtab.nochim))), sum)\nreads.per.seqlen\n\ntable_reads_seqlen = data.frame(length=as.numeric(names(reads.per.seqlen)), count=reads.per.seqlen)\nggplot(data=table_reads_seqlen, aes(x=length, y=count)) + geom_col()\n```\n:::\n\n\nNow, we have to filter the length of the ASVs. For that purpose, we have to keep in mind the data from the dataframe and the histogram. Choose those lengths in which we can retrieve enough sequences (\\>10000 sequences, if possible)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseqtab.nochim = seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% seq(401,428)] #the consensus sequences of our ASVs will be from 401 to 428 nucleotides\n```\n:::\n\n\n# 12. Taxonomic classification\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxa_rdp2 =assignTaxonomy(seqtab.nochim, \"/home/databases/rdp_train_set_19_H.fa\", multithread=TRUE)\n#we selected the last version of the RDP-II training set v.19\n\nASV = seqtab.nochim #edit the tables\nASVt = t(ASV)\n\ntaxa_rdp_na = apply(taxa_rdp2,2, tidyr::replace_na, \"unclassified\")[,-7] #replace the \"NA\" values by \"unclassified\" in those case in which an ASV have not been identified at a specific taxonomic rank\ntaxa_rdp_na=taxa_rdp_na[,-(7:15)]\n\n#we are going to modify a bit the tables so that the name of ASVs looks like \"ASV00001\" (in case we have 10000 ASVs or more). In case we have just 100 ASVs, they will have that code: \"ASV001\"\n\nnumber.digit = nchar(as.integer(nrow(ASVt)))\nnames =paste0(\"ASV%0\", number.digit, \"d\") #As many 0 as digits\nASV_names<- sprintf(names, 1:nrow(ASVt))\n\nASV_table_classified_raw = cbind(as.data.frame(taxa_rdp_na,stringsAsFactors = FALSE),as.data.frame(ASV_names, stringsAsFactors = FALSE),as.data.frame(ASVt,stringsAsFactors = FALSE))\n\nASV_seqs = rownames(ASV_table_classified_raw)\nrownames(ASV_table_classified_raw) <- NULL\nASV_table_classified_raw = cbind(ASV_seqs, ASV_table_classified_raw)\n\nwrite.table(ASV_table_classified_raw, file=\"ASV_table_classified_raw_wirhMockwithPlastids.txt\", sep=\"\\t\")\n```\n:::\n\n\nNow, we have our ASV table with the corresponding taxonomic classification. But, it still includes mitochondria and plastids from the host plants, the mock community and other invalid sequences.\n\n# 13. MOCK Community: setting the sequencing detection limit\n\nWe included a mock community in order to establish the detection limit of the sequencing process. Now, we have to address this cut-off. For that purpose, we will run the function *MockCommunity* which will ask us whether specific microorganisms are included in the mock community. We have to answer accordingly, and the function will go on asking us until an ASV not included in the mock community is found. Its relative abundance in the whole dataset will be considered as the sequencing detection limit, since it should have not been detected if it was not included in the mock community (a false detection, so all the ASV whose relative abundance is below this cut-off, are not real ASVs). Then, we have to remove the mock community.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nASV_filtered_MOCK=MockCommunity(ASV_table_classified_raw,mock_composition,ASV_column = \"ASV_names\")\n```\n:::\n\n\n::: callout-caution\nBe careful with ASVs belonging to genus *Limosilactobacillus* (especially if their relative abundance is high). Possibly, they are missclassified and belong to genus *Bacillus*, which is a member of the MockCommunity here employed.\n:::\n\n# 14. Removal of erronous sequences and taxonomy refinement\n\nThis step is especially important if we are sequencing plant tissues (root, phyllosphere, etc), because with the primers employed here it is possible to amplify plant hosts' DNA, for example, that corresponding to mitochondria, chloroplasts, among others. Thus, let's remove all this undesired sequences. We will also remove those ASVs not classified even at kingdom level:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nASV_final=ASV_filtered_MOCK[(which(ASV_filtered_MOCK$Genus!=\"Streptophyta\"  &ASV_filtered_MOCK$Genus!=\"Mitochondria\" &  ASV_filtered_MOCK$Genus!=\"Chlorophyta\"  & ASV_filtered_MOCK$Genus!=\"Bacillariophyta\" & ASV_filtered_MOCK$Family!=\"Streptophyta\"  & ASV_filtered_MOCK$Family!=\"Chlorophyta\" & ASV_filtered_MOCK$Family!=\"Bacillariophyta\"  & ASV_filtered_MOCK$Family!=\"Mitochondria\" & ASV_filtered_MOCK$Class!=\"Chloroplast\"  & ASV_filtered_MOCK$Order!=\"Chloroplast\" &ASV_filtered_MOCK$Family!=\"Chloroplast\"  & ASV_filtered_MOCK$Kingdom!=\"Eukaryota\" & ASV_filtered_MOCK$Kingdom!=\"unclassified\")),]\n```\n:::\n\n\nCyanobacterial sequences could be tricky, since they could be really close to plant sequences (chloroplasts). Thus, we should retrieve those sequences adscribed to the phylym *Cyanobacteriota*, and we then remove those that are not classified at class level. In our case, all the Cyanobacteria were chloroplasts (Phylum *Cyanobacteriota*, class Chloroplast), hence, they were removed in the previous step. But we encourage to make a BLAST at this point\n\nWe are going to check also the ASVs not classified at Phylum level\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunclass_phy = ASV_final[which(ASV_final$Phylum==\"unclassified\"),] #save them into a new variable\nseq_unclass = as.list(unclass_phy$ASV_seqs)\nwrite.fasta(seq_unclass, names=unclass_phy$ASV_names, file.out=\"unclassified_phylum.fas\", open = \"w\", nbchar =1000 , as.string = FALSE) #write the sequences in fasta format\n```\n:::\n\n\nNow, we run align by BLASTn these unclassified sequences against those held in the NCBI nt database, which is already downloaded in our PC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem((\"/home/programas/ncbi-blast-2.13.0+/bin/blastn -query unclassified_phylum.fas -db /home/databases/nt -out unclassified_phylum_hits.txt -outfmt '6 std stitle' -show_gis -max_target_seqs 5 -parse_deflines -num_threads 10\"),intern = TRUE)\n\n#we checked manually the output of BLAST and removed one ASV (ASV01487, unclassified at Phylum level with the RDP-II database) which was classified as *Musa* sp.\n\nASV_final_all_filters=ASV_final[(which(ASV_final$ASV_names!=\"ASV01487\")),]\n```\n:::\n\n\nHere we removed the sample corresponding to the negative control of the sequencing run (named \"Cneg\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nASV_final_all_filters=subset(ASV_final_all_filters, select = -c(Cneg))\n```\n:::\n\n\nAnd finally, let's save the definitive bacterial ASV table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.table(data.frame(\" \"=rownames(ASV_final_all_filters),ASV_final_all_filters),file=\"ASV_Bacterias_final.txt\", sep=\"\\t\",row.names =F)\n```\n:::\n\n\nWe will also save the fasta file and perform a phylogenetic tree with **all the ASVs** because then we will need a phylogenetic tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseq_fasta = as.list(ASV_final_all_filters$ASV_seqs) \nwrite.fasta(seq_fasta, names=ASV_final_all_filters$ASV_names, file.out=\"fasta_para_arbol16S.fas\", open = \"w\", nbchar =1000 , as.string = FALSE)\n```\n:::\n\n\n# 15. Extra code\n\nAs stated before, we will need a phylogenetic tree to calculate Weighted UniFrac distances among samples. Thus, here you have the code needed to calculate the tree (supposing you will calculate it in the save working path in which you have saved the fasta file *seq_fasta*)\n\nFirstly, we have to align all the sequences. For that purpose, we will use [MAFFT software](https://mafft.cbrc.jp/alignment/server/index.html \"Click to MAFFT website\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmafft = \"/usr/bin/mafft\" #set MAFFT's path\n\nsystem2(mafft, args=c(\"--auto\", \"fasta_para_arbol16S.fas>\", \"alignment\"))#after \"--auto\", set the name of the fasta file, followed by the name of the alignment.\n```\n:::\n\n\nNow, we are going to calculate the phylogenetic tree with [FastTree MP tool](http://www.microbesonline.org/fasttree/ \"Click to FastTree website\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFastTreeMP= \"/home/programas/FastTreeMP/FastTreeMP\"#set the path of the software\n\nsystem2(FastTreeMP, args=\"--version\" )\nsystem2(FastTreeMP, args = c(\"-gamma\", \"-nt\", \"-gtr\", \"-spr\",4 ,\"-mlacc\", 2, \"-slownni\", \"<alignment>\", \"tree\"))\n\n#Our tree is Gamma20-based likelihood tree, based on the input alignment named \"alignment\", and the output tree is named \"tree\".\n```\n:::\n\n\nClick [here](Bacteria_data_analysis.qmd \"Bacterial data analyses\") to visit the scripts needed for the ecological analyses\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}